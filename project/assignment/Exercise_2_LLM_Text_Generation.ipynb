{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b12430",
   "metadata": {},
   "source": [
    "\n",
    "# Loading and Experimenting with a Pre-trained Large Language Model (LLM)\n",
    "\n",
    "In this exercise, we will load a pre-trained Large Language Model (LLM) from Hugging Face and experiment with it to generate text. We'll use GPT-2, a popular LLM, and explore how to generate different text outputs based on user prompts.\n",
    "\n",
    "### Steps:\n",
    "1. Install the necessary libraries.\n",
    "2. Load the GPT-2 model from Hugging Face.\n",
    "3. Generate text based on user input.\n",
    "4. Experiment with different prompts and parameters (such as length and temperature) to see how they affect the generated text.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd678c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install the required libraries\n",
    "!pip install transformers torch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf6b2f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Load the GPT-2 Model and Tokenizer\n",
    "\n",
    "We'll load the GPT-2 model and tokenizer from the Hugging Face Transformers library.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb57fe",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Text Generation with GPT-2\n",
    "\n",
    "Now, we'll create a function that takes a user input as a prompt and generates text using the GPT-2 model. You can modify the prompt and parameters like `max_length` and `temperature` to experiment with different outputs.\n",
    "The `generate_text` function takes the following inputs:\n",
    "- `prompt`: A string input that serves as the starting point for text generation.\n",
    "- `max_length`: The maximum length of the generated text (default is 50).\n",
    "- `temperature`: A parameter that controls the randomness or creativity of the generated text (default is 1.0, higher values result in more randomness).\n",
    "- `num_return_sequences=1`: Specifies that only one sequence of text should be generated.\n",
    "- `no_repeat_ngram_size=2`: Ensures that no 2-grams (two consecutive words) are repeated in the generated text.\n",
    "- `do_sample=True`: Enables sampling, allowing the model to explore different possible continuations for the text.\n",
    "\n",
    "\n",
    "After the text is generated, the function decodes the tokenized output back into human-readable text.\n",
    "\n",
    "```python\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "- `tokenizer.decode(outputs[0])`: Converts the tokenized output back into a string.\n",
    "- `skip_special_tokens=True`: Removes any special tokens that the model might have added during generation (such as end-of-sequence tokens).\n",
    "\n",
    "Finally, the decoded text is returned as the output of the function.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f088da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(prompt, max_length=50, temperature=1.0):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length, \n",
    "        temperature=temperature, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74087f82",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Experiment with Different Prompts\n",
    "\n",
    "We will now create an interactive Gradio interface where you can input your own prompts and generate text using the GPT-2 model. You can also adjust the `max_length` (number of tokens generated) and `temperature` (controls the randomness of the output).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa27e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Create the Gradio interface\n",
    "def generate_interface(prompt, max_length, temperature):\n",
    "    return generate_text(prompt, max_length=max_length, temperature=temperature)\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=generate_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter your prompt here\", label=\"Prompt\"),\n",
    "        gr.Slider(10, 100, value=50, label=\"Max Length\"),\n",
    "        gr.Slider(0.7, 1.3, value=1.0, label=\"Temperature\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Text Generation with GPT-2\",\n",
    "    description=\"Enter a prompt and experiment with different parameters to generate text.\"\n",
    ")\n",
    "\n",
    "interface.launch()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

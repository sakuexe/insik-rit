{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In the previous Exercise, we discussed a model that had two convolutional blocks and dropout layers. However, that model is still quite basic compared to truly advanced models. Modern state-of-the-art models have tens of convolutional blocks, millions of parameters, and leverage sophisticated architectural techniques. These models are trained on massive datasets, requiring thousands of hours of GPU time, something most of us don't have the resources for.\n",
    "\n",
    "So, what’s the solution for those of us without such resources? Enter **Transfer Learning**.\n",
    "\n",
    "The concept behind transfer learning is simple. Large companies with access to immense data and computational power develop massive models and train them on vast datasets. Once these models are trained, the architecture and pre-trained weights are shared with the public, allowing others to use these models as a starting point for their own tasks. The pre-trained weights serve as the foundation, and we fine-tune them to suit our specific needs.\n",
    "\n",
    "One of the most widely known datasets used for training such models is **ImageNet**, which contains over 14 million images across 27 high-level categories and 20,000 sub-categories. Although the images cannot be directly downloaded from the ImageNet site due to copyright issues, the URLs are provided.\n",
    "\n",
    "In the early 2010s, ImageNet classification was a monumental task, leading to the creation of many famous architectures like **AlexNet**, **VGG**, **Inception**, and **ResNet**. These architectures revolutionized computer vision, and many of them were developed during the **ILSVRC** (ImageNet Large Scale Visual Recognition Challenge), which ran from 2010 to 2017.\n",
    "\n",
    "In this notebook, we will explore **ResNet**, one of the well-known architectures from the ILSVRC competition. We will revisit the **Rock Paper Scissors** dataset we used in last exercise. But this time, instead of building our own model, we'll take ResNet for a spin using PyTorch and its pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "\n",
    "Before we begin working on our transfer learning task, we need to import several key libraries:\n",
    "\n",
    "- **PyTorch (`torch`)**: The core deep learning library used to build and train models.\n",
    "- **`torch.nn`**: A module that provides neural network layers and operations, which will be used to define and modify our model.\n",
    "- **`torch.optim`**: A module that provides optimization algorithms, such as SGD and Adam, for training the model.\n",
    "- **`torchvision.models`**: A collection of pre-trained models, including AlexNet, which we will be using for transfer learning.\n",
    "- **`torchvision.transforms`**: A set of common transformations, such as resizing and converting images to tensors, to preprocess our image dataset.\n",
    "- **`torchvision.datasets.ImageFolder`**: A utility to load and structure image datasets stored in folders.\n",
    "- **`torch.utils.data.DataLoader`**: A utility to load data in batches and shuffle it for training.\n",
    "- **`wandb`**: Weights & Biases, a tool for tracking and visualizing machine learning experiments.\n",
    "\n",
    "In this block, we’re setting up all the necessary imports to get started with loading a pre-trained model and preparing our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "# import wandb  # Import Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing Weights & Biases (W&B) for Experiment Tracking\n",
    "\n",
    "In this block, we initialize **Weights & Biases (W&B)** to track and log our experiment. W&B is a popular tool used in machine learning for tracking model performance, visualizing training progress, and organizing results.\n",
    "\n",
    "- **`wandb.init()`**: This initializes the W&B project and sets the configuration for the experiment.\n",
    "    - **`project`**: The name of the project on W&B. In this case, it’s called `\"resnet18-transfer-learning-hamk\"`, indicating that we will be using the ResNet18 model for transfer learning.\n",
    "    - **`config`**: A dictionary that defines key parameters of our experiment:\n",
    "        - **`epochs`**: The number of training epochs (in this case, 10).\n",
    "        - **`batch_size`**: The size of each batch during training (set to 16).\n",
    "        - **`learning_rate`**: The initial learning rate for the optimizer (set to 0.001).\n",
    "        - **`architecture`**: The pre-trained model architecture we will use (ResNet18).\n",
    "        - **`dataset`**: The name of the dataset (Rock Paper Scissors, abbreviated as \"RPS\").\n",
    "\n",
    "By using W&B, we can easily monitor our model’s performance, visualize metrics such as loss and accuracy, and compare different runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uywbhi8p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7b7cef26f2403cb75693bf347d274b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.010 MB of 0.010 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▁▇██</td></tr><tr><td>Train Loss</td><td>▇█▃▁▁</td></tr><tr><td>Validation Accuracy</td><td>▅██▆▁</td></tr><tr><td>Validation Loss</td><td>▃▁▂▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>99.44444</td></tr><tr><td>Train Loss</td><td>0.02823</td></tr><tr><td>Validation Accuracy</td><td>75.80645</td></tr><tr><td>Validation Loss</td><td>0.56435</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-music-1</strong> at: <a href='https://wandb.ai/HAMKca/resnet18-transfer-learning/runs/uywbhi8p' target=\"_blank\">https://wandb.ai/HAMKca/resnet18-transfer-learning/runs/uywbhi8p</a><br/> View project at: <a href='https://wandb.ai/HAMKca/resnet18-transfer-learning' target=\"_blank\">https://wandb.ai/HAMKca/resnet18-transfer-learning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241004_155509-uywbhi8p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uywbhi8p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\s1006\\Desktop\\Teaching 2024\\Solutions in Pattern Recognition\\Simple_CNN\\wandb\\run-20241004_160232-w7az38uk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk/runs/w7az38uk' target=\"_blank\">deep-waterfall-1</a></strong> to <a href='https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk' target=\"_blank\">https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk/runs/w7az38uk' target=\"_blank\">https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk/runs/w7az38uk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/HAMKca/resnet18-transfer-learning-hamk/runs/w7az38uk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x18392503950>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize W&B and set up the project\n",
    "wandb.init(project=\"resnet18-transfer-learning-hamk\", config={\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"ResNet18\",\n",
    "    \"dataset\": \"RPS\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Dataset and Data Loaders\n",
    "\n",
    "In this block, we define the necessary transformations for our dataset, load the data, and create data loaders for training and validation.\n",
    "\n",
    "### Transformations\n",
    "- **`Compose()`**: We use `Compose` to apply a series of transformations to the images.\n",
    "    - **`Resize((28, 28))`**: This resizes the images to a fixed size of 28x28 pixels, which is suitable for the model input.\n",
    "    - **`ToTensor()`**: This converts the images into tensors, which are required by PyTorch for model training.\n",
    "\n",
    "### Loading the Dataset\n",
    "- **`ImageFolder()`**: This utility is used to load images stored in folder structures. Each folder corresponds to a class label, and the images are automatically labeled based on the folder names.\n",
    "    - **`train_data`**: The training dataset is loaded from the `\"rps\"` directory, which contains images for Rock Paper Scissors.\n",
    "    - **`val_data`**: The validation dataset is loaded from the `\"rps-test-set\"` directory.\n",
    "    - **`train_data.classes`**: This prints out the class labels (Rock, Paper, Scissors) from the dataset.\n",
    "\n",
    "### Data Loaders\n",
    "- **`DataLoader()`**: This utility loads data in batches and shuffles it for training or validation.\n",
    "    - **`train_loader`**: Loads the training dataset with a batch size of 16 and shuffles the data.\n",
    "    - **`val_loader`**: Loads the validation dataset with a batch size of 16 but without shuffling.\n",
    "\n",
    "The final lines of the block print the number of samples in both the training and validation sets. These data loaders will be used to feed data into the model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Number of training samples: 2520\n",
      "Number of validation samples: 372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformations (from your lab)\n",
    "transform = Compose([Resize((28, 28)), ToTensor()])\n",
    "\n",
    "# Load the dataset\n",
    "train_data = ImageFolder(root='rps', transform=transform)\n",
    "val_data = ImageFolder(root='rps-test-set', transform=transform)\n",
    "print('Classes:', train_data.classes)\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: rps-test-set\n",
      "Number of files: 0\n",
      "Files: []\n",
      "--------------------------------------------------\n",
      "Directory: rps-test-set/paper\n",
      "Number of files: 124\n",
      "Files: ['testpaper01-00.png', 'testpaper01-01.png', 'testpaper01-02.png', 'testpaper01-03.png', 'testpaper01-04.png', 'testpaper01-05.png', 'testpaper01-06.png', 'testpaper01-07.png', 'testpaper01-08.png', 'testpaper01-09.png', 'testpaper01-10.png', 'testpaper01-11.png', 'testpaper01-12.png', 'testpaper01-13.png', 'testpaper01-14.png', 'testpaper01-15.png', 'testpaper01-16.png', 'testpaper01-17.png', 'testpaper01-18.png', 'testpaper01-19.png', 'testpaper01-20.png', 'testpaper01-21.png', 'testpaper01-22.png', 'testpaper01-23.png', 'testpaper01-24.png', 'testpaper01-25.png', 'testpaper01-26.png', 'testpaper01-27.png', 'testpaper01-28.png', 'testpaper01-29.png', 'testpaper01-30.png', 'testpaper02-00.png', 'testpaper02-01.png', 'testpaper02-02.png', 'testpaper02-03.png', 'testpaper02-04.png', 'testpaper02-05.png', 'testpaper02-06.png', 'testpaper02-07.png', 'testpaper02-08.png', 'testpaper02-09.png', 'testpaper02-10.png', 'testpaper02-11.png', 'testpaper02-12.png', 'testpaper02-13.png', 'testpaper02-14.png', 'testpaper02-15.png', 'testpaper02-16.png', 'testpaper02-17.png', 'testpaper02-18.png', 'testpaper02-19.png', 'testpaper02-20.png', 'testpaper02-21.png', 'testpaper02-22.png', 'testpaper02-23.png', 'testpaper02-24.png', 'testpaper02-25.png', 'testpaper02-26.png', 'testpaper02-27.png', 'testpaper02-28.png', 'testpaper02-29.png', 'testpaper02-30.png', 'testpaper03-00.png', 'testpaper03-01.png', 'testpaper03-02.png', 'testpaper03-03.png', 'testpaper03-04.png', 'testpaper03-05.png', 'testpaper03-06.png', 'testpaper03-07.png', 'testpaper03-08.png', 'testpaper03-09.png', 'testpaper03-10.png', 'testpaper03-11.png', 'testpaper03-12.png', 'testpaper03-13.png', 'testpaper03-14.png', 'testpaper03-15.png', 'testpaper03-16.png', 'testpaper03-17.png', 'testpaper03-18.png', 'testpaper03-19.png', 'testpaper03-20.png', 'testpaper03-21.png', 'testpaper03-22.png', 'testpaper03-23.png', 'testpaper03-24.png', 'testpaper03-25.png', 'testpaper03-26.png', 'testpaper03-27.png', 'testpaper03-28.png', 'testpaper03-29.png', 'testpaper03-30.png', 'testpaper04-00.png', 'testpaper04-01.png', 'testpaper04-02.png', 'testpaper04-03.png', 'testpaper04-04.png', 'testpaper04-05.png', 'testpaper04-06.png', 'testpaper04-07.png', 'testpaper04-08.png', 'testpaper04-09.png', 'testpaper04-10.png', 'testpaper04-11.png', 'testpaper04-12.png', 'testpaper04-13.png', 'testpaper04-14.png', 'testpaper04-15.png', 'testpaper04-16.png', 'testpaper04-17.png', 'testpaper04-18.png', 'testpaper04-19.png', 'testpaper04-20.png', 'testpaper04-21.png', 'testpaper04-22.png', 'testpaper04-23.png', 'testpaper04-24.png', 'testpaper04-25.png', 'testpaper04-26.png', 'testpaper04-27.png', 'testpaper04-28.png', 'testpaper04-29.png', 'testpaper04-30.png']\n",
      "--------------------------------------------------\n",
      "Directory: rps-test-set/rock\n",
      "Number of files: 124\n",
      "Files: ['testrock01-00.png', 'testrock01-01.png', 'testrock01-02.png', 'testrock01-03.png', 'testrock01-04.png', 'testrock01-05.png', 'testrock01-06.png', 'testrock01-07.png', 'testrock01-08.png', 'testrock01-09.png', 'testrock01-10.png', 'testrock01-11.png', 'testrock01-12.png', 'testrock01-13.png', 'testrock01-14.png', 'testrock01-15.png', 'testrock01-16.png', 'testrock01-17.png', 'testrock01-18.png', 'testrock01-19.png', 'testrock01-20.png', 'testrock01-21.png', 'testrock01-22.png', 'testrock01-23.png', 'testrock01-24.png', 'testrock01-25.png', 'testrock01-26.png', 'testrock01-27.png', 'testrock01-28.png', 'testrock01-29.png', 'testrock01-30.png', 'testrock02-00.png', 'testrock02-01.png', 'testrock02-02.png', 'testrock02-03.png', 'testrock02-04.png', 'testrock02-05.png', 'testrock02-06.png', 'testrock02-07.png', 'testrock02-08.png', 'testrock02-09.png', 'testrock02-10.png', 'testrock02-11.png', 'testrock02-12.png', 'testrock02-13.png', 'testrock02-14.png', 'testrock02-15.png', 'testrock02-16.png', 'testrock02-17.png', 'testrock02-18.png', 'testrock02-19.png', 'testrock02-20.png', 'testrock02-21.png', 'testrock02-22.png', 'testrock02-23.png', 'testrock02-24.png', 'testrock02-25.png', 'testrock02-26.png', 'testrock02-27.png', 'testrock02-28.png', 'testrock02-29.png', 'testrock02-30.png', 'testrock03-00.png', 'testrock03-01.png', 'testrock03-02.png', 'testrock03-03.png', 'testrock03-04.png', 'testrock03-05.png', 'testrock03-06.png', 'testrock03-07.png', 'testrock03-08.png', 'testrock03-09.png', 'testrock03-10.png', 'testrock03-11.png', 'testrock03-12.png', 'testrock03-13.png', 'testrock03-14.png', 'testrock03-15.png', 'testrock03-16.png', 'testrock03-17.png', 'testrock03-18.png', 'testrock03-19.png', 'testrock03-20.png', 'testrock03-21.png', 'testrock03-22.png', 'testrock03-23.png', 'testrock03-24.png', 'testrock03-25.png', 'testrock03-26.png', 'testrock03-27.png', 'testrock03-28.png', 'testrock03-29.png', 'testrock03-30.png', 'testrock04-00.png', 'testrock04-01.png', 'testrock04-02.png', 'testrock04-03.png', 'testrock04-04.png', 'testrock04-05.png', 'testrock04-06.png', 'testrock04-07.png', 'testrock04-08.png', 'testrock04-09.png', 'testrock04-10.png', 'testrock04-11.png', 'testrock04-12.png', 'testrock04-13.png', 'testrock04-14.png', 'testrock04-15.png', 'testrock04-16.png', 'testrock04-17.png', 'testrock04-18.png', 'testrock04-19.png', 'testrock04-20.png', 'testrock04-21.png', 'testrock04-22.png', 'testrock04-23.png', 'testrock04-24.png', 'testrock04-25.png', 'testrock04-26.png', 'testrock04-27.png', 'testrock04-28.png', 'testrock04-29.png', 'testrock04-30.png']\n",
      "--------------------------------------------------\n",
      "Directory: rps-test-set/scissors\n",
      "Number of files: 124\n",
      "Files: ['testscissors01-00.png', 'testscissors01-01.png', 'testscissors01-02.png', 'testscissors01-03.png', 'testscissors01-04.png', 'testscissors01-05.png', 'testscissors01-06.png', 'testscissors01-07.png', 'testscissors01-08.png', 'testscissors01-09.png', 'testscissors01-10.png', 'testscissors01-11.png', 'testscissors01-12.png', 'testscissors01-13.png', 'testscissors01-14.png', 'testscissors01-15.png', 'testscissors01-16.png', 'testscissors01-17.png', 'testscissors01-18.png', 'testscissors01-19.png', 'testscissors01-20.png', 'testscissors01-21.png', 'testscissors01-22.png', 'testscissors01-23.png', 'testscissors01-24.png', 'testscissors01-25.png', 'testscissors01-26.png', 'testscissors01-27.png', 'testscissors01-28.png', 'testscissors01-29.png', 'testscissors01-30.png', 'testscissors02-00.png', 'testscissors02-01.png', 'testscissors02-02.png', 'testscissors02-03.png', 'testscissors02-04.png', 'testscissors02-05.png', 'testscissors02-06.png', 'testscissors02-07.png', 'testscissors02-08.png', 'testscissors02-09.png', 'testscissors02-10.png', 'testscissors02-11.png', 'testscissors02-12.png', 'testscissors02-13.png', 'testscissors02-14.png', 'testscissors02-15.png', 'testscissors02-16.png', 'testscissors02-17.png', 'testscissors02-18.png', 'testscissors02-19.png', 'testscissors02-20.png', 'testscissors02-21.png', 'testscissors02-22.png', 'testscissors02-23.png', 'testscissors02-24.png', 'testscissors02-25.png', 'testscissors02-26.png', 'testscissors02-27.png', 'testscissors02-28.png', 'testscissors02-29.png', 'testscissors02-30.png', 'testscissors03-00.png', 'testscissors03-01.png', 'testscissors03-02.png', 'testscissors03-03.png', 'testscissors03-04.png', 'testscissors03-05.png', 'testscissors03-06.png', 'testscissors03-07.png', 'testscissors03-08.png', 'testscissors03-09.png', 'testscissors03-10.png', 'testscissors03-11.png', 'testscissors03-12.png', 'testscissors03-13.png', 'testscissors03-14.png', 'testscissors03-15.png', 'testscissors03-16.png', 'testscissors03-17.png', 'testscissors03-18.png', 'testscissors03-19.png', 'testscissors03-20.png', 'testscissors03-21.png', 'testscissors03-22.png', 'testscissors03-23.png', 'testscissors03-24.png', 'testscissors03-25.png', 'testscissors03-26.png', 'testscissors03-27.png', 'testscissors03-28.png', 'testscissors03-29.png', 'testscissors03-30.png', 'testscissors04-00.png', 'testscissors04-01.png', 'testscissors04-02.png', 'testscissors04-03.png', 'testscissors04-04.png', 'testscissors04-05.png', 'testscissors04-06.png', 'testscissors04-07.png', 'testscissors04-08.png', 'testscissors04-09.png', 'testscissors04-10.png', 'testscissors04-11.png', 'testscissors04-12.png', 'testscissors04-13.png', 'testscissors04-14.png', 'testscissors04-15.png', 'testscissors04-16.png', 'testscissors04-17.png', 'testscissors04-18.png', 'testscissors04-19.png', 'testscissors04-20.png', 'testscissors04-21.png', 'testscissors04-22.png', 'testscissors04-23.png', 'testscissors04-24.png', 'testscissors04-25.png', 'testscissors04-26.png', 'testscissors04-27.png', 'testscissors04-28.png', 'testscissors04-29.png', 'testscissors04-30.png']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "val_dir = 'rps-test-set'\n",
    "for root, dirs, files in os.walk(val_dir):\n",
    "    print(f\"Directory: {root}\")\n",
    "    print(f\"Number of files: {len(files)}\")\n",
    "    print(f\"Files: {files}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Pre-trained ResNet Model\n",
    "\n",
    "In this block, we load the pre-trained **ResNet18** model, which is a popular deep learning architecture known for its residual connections that help avoid the vanishing gradient problem in deep networks.\n",
    "\n",
    "- **`models.resnet18()`**: This function loads the **ResNet18** architecture from the **torchvision** library.\n",
    "    - **`weights=models.ResNet18_Weights.DEFAULT`**: This specifies that we are using the pre-trained weights provided by **torchvision**. These weights are trained on a large-scale dataset like ImageNet, allowing us to benefit from a model that has already learned useful features.\n",
    "- **`.to(device)`**: This moves the model to the specified device (usually a GPU or CPU). If a GPU is available, the model will run on it for faster computations.\n",
    "\n",
    "By using this pre-trained model, we leverage the knowledge it has already learned and adapt it to our specific task (transfer learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/sakuk/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 21.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained ResNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchsummary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Displaying the Model Summary\n",
    "\n",
    "In this block, we use the **torchsummary** library to display a detailed summary of the **ResNet18** model architecture. This helps us understand the structure of the model and the number of parameters involved.\n",
    "\n",
    "- **`summary(model, input_size=(3, 224, 224))`**: This function provides a layer-by-layer summary of the model architecture.\n",
    "    - **Input size**: For ResNet18, the expected input size is a 3-channel image (RGB) with dimensions 224x224 pixels.\n",
    "\n",
    "### Model Summary\n",
    "- **Layer (type)**: Lists each layer in the model, such as convolutional layers, batch normalization, ReLU activations, and fully connected layers.\n",
    "- **Output Shape**: Displays the shape of the output after each layer.\n",
    "- **Param #**: Shows the number of parameters in each layer, including trainable parameters like weights and biases.\n",
    "\n",
    "The model has approximately **11.7 million parameters**, all of which are trainable. This is a relatively small number compared to deeper models like ResNet50, making ResNet18 a good choice for transfer learning tasks with limited computational resources.\n",
    "\n",
    "The summary provides the estimated memory usage for the input size, forward/backward passes, and the parameters, giving us a clear idea of the model's complexity and memory requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 107.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Display model summary (for an input size of (3, 224, 224) for ResNet)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modifying the Final Fully Connected Layer for Transfer Learning\n",
    "\n",
    "In transfer learning, we often modify the final layer of a pre-trained model to adapt it to our specific task. In this case, we are fine-tuning the **ResNet18** model for the **Rock-Paper-Scissors** dataset, which has 3 output classes.\n",
    "\n",
    "- **`model.fc = nn.Linear(512, 3)`**: \n",
    "    - The final fully connected (FC) layer of ResNet18 is originally designed for 1000 classes (as it was trained on ImageNet).\n",
    "    - We replace the FC layer with a new `Linear` layer that has 512 input features (from the last layer of ResNet18) and 3 output features (corresponding to the 3 classes: Rock, Paper, and Scissors).\n",
    "    - **`.to(device)`**: Moves the modified layer to the device (GPU or CPU).\n",
    "\n",
    "By modifying this layer, we retain all the features learned by the pre-trained model and fine-tune the final layer to classify images into the 3 target categories.\n",
    "\n",
    "- The following image is of AlexNet but the idea of Modifying the final layer is same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/alexnet.png?raw=1)\n",
    "\n",
    "*Source: Generated using Alexander Lenail’s [NN-SVG](http://alexlenail.me/NN-SVG/) and adapted by the author*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify the final fully connected layer for transfer learning (assuming 3 classes for the Rock-Paper-Scissors dataset)\n",
    "model.fc = nn.Linear(512, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Defining the Training and Validation Loops\n",
    "\n",
    "In this section, we define two key functions: one for training the model and one for validating its performance. These loops are fundamental to the transfer learning process, where the pre-trained model is fine-tuned on our specific dataset.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "The **`train()`** function handles the training process for one epoch, performing the following steps:\n",
    "\n",
    "- **`model.train()`**: Sets the model to training mode, enabling features like dropout and batch normalization.\n",
    "- **Forward pass**: The input images are passed through the model, and the output predictions are generated.\n",
    "- **Loss calculation**: The loss between the model's predictions and the true labels is computed using the specified criterion (e.g., cross-entropy loss).\n",
    "- **Backward pass**: The gradients of the loss with respect to the model parameters are computed (via `loss.backward()`), and the optimizer updates the model weights.\n",
    "- **Accuracy calculation**: The number of correctly predicted labels is compared against the total number of labels to calculate the training accuracy.\n",
    "- **W&B logging**: The loss and accuracy for the current epoch are logged to **Weights & Biases** (W&B), allowing us to track the model's performance.\n",
    "\n",
    "### Validation Loop\n",
    "\n",
    "The **`validate()`** function handles the evaluation of the model on the validation set:\n",
    "\n",
    "- **`model.eval()`**: Sets the model to evaluation mode, disabling certain features like dropout.\n",
    "- **No gradient calculation**: The validation loop is wrapped in `torch.no_grad()` to prevent gradient calculations and save memory during the evaluation.\n",
    "- **Forward pass**: Similar to the training loop, the input images are passed through the model, but no backpropagation or optimization is performed.\n",
    "- **Accuracy and loss calculation**: The loss and accuracy are computed for the validation set.\n",
    "- **W&B logging**: The validation loss and accuracy are logged to W&B to monitor the model's performance on unseen data.\n",
    "\n",
    "Both loops track key metrics, including loss and accuracy, and use **Weights & Biases** to log these metrics for real-time monitoring of the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "\n",
    "    # Log the metrics to W&B\n",
    "    # wandb.log({\"Train Loss\": epoch_loss, \"Train Accuracy\": epoch_accuracy})\n",
    "    print(f\"Train Loss: {epoch_loss}, Train Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "# Validation loop\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    # Log the metrics to W&B\n",
    "    # wandb.log({\"Validation Loss\": epoch_loss, \"Validation Accuracy\": epoch_accuracy})\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_loss}, Validation Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Model\n",
    "\n",
    "In this block, we train the model for a set number of epochs using the previously defined **`train()`** and **`validate()`** functions. The model will learn from the training data and be evaluated on the validation set at the end of each epoch.\n",
    "\n",
    "- **`num_epochs = 10`**: Specifies that the model will be trained for 10 epochs. This can be adjusted based on the dataset size, model complexity, and available resources.\n",
    "- **Training Loop**: \n",
    "    - For each epoch, the **`train()`** function is called to train the model on the training set.\n",
    "    - After each epoch, the **`validate()`** function evaluates the model on the validation set to track its performance.\n",
    "    - For each epoch, the loss and accuracy for both the training and validation sets are printed to monitor progress.\n",
    "\n",
    "### Weights & Biases (W&B) Logging\n",
    "- **`wandb.watch(model)`**: This function allows **W&B** to track the model's gradients and parameters throughout training, enabling more detailed insights and visualizations into the model’s training process.\n",
    "\n",
    "By logging each epoch's results and using **W&B** to track the model’s progress, we ensure a well-documented and transparent training process. After training, the model can be saved and used for future inference or further fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.0652164615894435, Train Accuracy: 98.69047619047619%\n",
      "Validation Loss: 0.21239544483129671, Validation Accuracy: 95.16129032258064%\n",
      "Epoch 2/10\n",
      "Train Loss: 0.0402599301743999, Train Accuracy: 99.08730158730158%\n",
      "Validation Loss: 0.08102831190626603, Validation Accuracy: 96.7741935483871%\n",
      "Epoch 3/10\n",
      "Train Loss: 0.0228103390228067, Train Accuracy: 99.8015873015873%\n",
      "Validation Loss: 0.6069893509084068, Validation Accuracy: 80.3763440860215%\n",
      "Epoch 4/10\n",
      "Train Loss: 0.003471090082643105, Train Accuracy: 99.96031746031746%\n",
      "Validation Loss: 0.0888495560308608, Validation Accuracy: 96.7741935483871%\n",
      "Epoch 5/10\n",
      "Train Loss: 0.00035359321910745383, Train Accuracy: 100.0%\n",
      "Validation Loss: 0.08173132430238184, Validation Accuracy: 96.50537634408602%\n",
      "Epoch 6/10\n",
      "Train Loss: 0.08537174668331145, Train Accuracy: 97.89682539682539%\n",
      "Validation Loss: 0.5953351118659876, Validation Accuracy: 74.19354838709677%\n",
      "Epoch 7/10\n",
      "Train Loss: 0.09185386070177766, Train Accuracy: 98.13492063492063%\n",
      "Validation Loss: 0.3297160410244639, Validation Accuracy: 87.09677419354838%\n",
      "Epoch 8/10\n",
      "Train Loss: 0.051707747334240835, Train Accuracy: 98.80952380952381%\n",
      "Validation Loss: 0.06607230355439242, Validation Accuracy: 98.65591397849462%\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0005262800144619983, Train Accuracy: 100.0%\n",
      "Validation Loss: 0.04029849636814712, Validation Accuracy: 99.19354838709677%\n",
      "Epoch 10/10\n",
      "Train Loss: 0.04762586992070236, Train Accuracy: 99.48412698412699%\n",
      "Validation Loss: 0.26603575102247606, Validation Accuracy: 89.24731182795699%\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 10 epochs (you can increase the epochs as needed)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    validate(model, val_loader, criterion, device)\n",
    "\n",
    "# Save the model to W&B\n",
    "# wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save the model to disk \n",
    "#torch.save(model.state_dict(), 'resnet18_transfer_learning.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: rock\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Function to perform inference on a single image\n",
    "def infer(model, image_path, transform, device):\n",
    "    # Load the image and convert it to RGB (3 channels)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = output.max(1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Class names are based on the order printed from train_data.classes\n",
    "class_names = ['paper', 'rock', 'scissors']\n",
    "\n",
    "# Path to the test image\n",
    "image_path = 'rps-test-set/rock/testrock01-00.png'  # Adjust the path as needed\n",
    "\n",
    "# Run inference\n",
    "predicted_class = infer(model, image_path, transform, device)\n",
    "print(f'Predicted class: {class_names[predicted_class]}')\n",
    "\n",
    "# Path to the test image\n",
    "# image_path = 'rps-test-set/rock/testrock01-00.png'  # Ensure the path is correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sakuk/code/pattern_recognizion/venv/lib64/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model with a Confusion Matrix\n",
    "\n",
    "In this block, we generate predictions for the entire validation set and display a **confusion matrix** to better understand how well the model is performing on each class (Rock, Paper, Scissors). The confusion matrix allows us to see how often the model correctly classifies each class and where it may be making errors.\n",
    "\n",
    "### Getting Predictions\n",
    "\n",
    "The **`get_all_predictions()`** function:\n",
    "- **`model.eval()`**: Sets the model to evaluation mode to disable training-specific features such as dropout.\n",
    "- **Predictions**: For each batch in the validation loader, we pass the inputs through the model and record the predicted class labels.\n",
    "    - The predictions are stored in **`all_preds`**, and the true labels in **`all_labels`**.\n",
    "    - Both predictions and labels are moved back to the CPU using `.cpu()` and then converted to numpy arrays for further processing.\n",
    "- **Concatenation**: After looping through the entire validation set, we concatenate the predictions and labels into single arrays for easy comparison.\n",
    "\n",
    "### Confusion Matrix\n",
    "- **`confusion_matrix()`**: This function from **scikit-learn** computes the confusion matrix by comparing the true labels with the predicted labels.\n",
    "- **`ConfusionMatrixDisplay()`**: This utility is used to visualize the confusion matrix.\n",
    "    - The confusion matrix shows how many times the model correctly or incorrectly predicted each class.\n",
    "    - **Class names**: The class names for the Rock, Paper, Scissors dataset are provided as `['paper', 'rock', 'scissors']`.\n",
    "    - The matrix is plotted using **`matplotlib`** with a blue color map to highlight the results.\n",
    "\n",
    "### Interpreting the Confusion Matrix\n",
    "- The confusion matrix helps us understand how the model is performing on each class.\n",
    "    - Diagonal entries represent correct predictions (true positives).\n",
    "    - Off-diagonal entries represent misclassifications (false positives/negatives).\n",
    "- By analyzing the confusion matrix, we can see if the model is struggling with any particular class or if there is a class imbalance in predictions.\n",
    "\n",
    "Finally, the confusion matrix is displayed as a plot using **matplotlib** to provide a visual overview of the model's classification performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 372\n",
      "Number of true labels: 372\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to get predictions for the entire validation set\n",
    "def get_all_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Convert list of arrays to single numpy arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Get predictions and true labels from validation data\n",
    "preds, labels = get_all_predictions(model, val_loader, device)\n",
    "# Print the shape of preds and labels\n",
    "print(f\"Number of predictions: {len(preds)}\")\n",
    "print(f\"Number of true labels: {len(labels)}\")\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# Display the confusion matrix\n",
    "class_names = ['paper', 'rock', 'scissors']  # Based on your dataset\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference on a Single Image and Logging Results to W&B\n",
    "\n",
    "In this block, we define a function to perform inference on a single image using the pre-trained model and log the results to **Weights & Biases (W&B)**. This is useful for evaluating the model's performance on individual images and visualizing the results.\n",
    "\n",
    "### Function: `infer_and_log()`\n",
    "\n",
    "- **Loading and Preprocessing**: \n",
    "    - **`Image.open(image_path).convert('RGB')`**: Opens the image from the given path and converts it to an RGB format (3-channel).\n",
    "    - **`transform(image)`**: Applies the transformation (e.g., resizing, tensor conversion) to the image, similar to how we transformed the training and validation images.\n",
    "    - **`.unsqueeze(0)`**: Adds a batch dimension to the image tensor, as the model expects input in batches.\n",
    "    - The image tensor is then moved to the specified device (GPU or CPU) using **`.to(device)`**.\n",
    "\n",
    "- **Inference**:\n",
    "    - **`model.eval()`**: The model is set to evaluation mode to disable any training-specific operations.\n",
    "    - **`torch.no_grad()`**: Disables gradient calculations to save memory and speed up inference.\n",
    "    - The model processes the image, and the predicted class is obtained by finding the index of the highest output score using **`.max(1)`**.\n",
    "    - The predicted class is then mapped to its corresponding label using the **`class_names`** list.\n",
    "\n",
    "- **Logging to Weights & Biases**:\n",
    "    - **`wandb.log()`**: Logs the original input image along with the predicted class to **W&B** for easy tracking and visualization.\n",
    "    - **`wandb.Image()`**: This function is used to log the input image along with a caption showing the predicted class.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "- **`class_names`**: The list of class names for the Rock, Paper, Scissors dataset (`['paper', 'rock', 'scissors']`).\n",
    "- **Image Path**: The path to a test image (e.g., `'rps-test-set/rock/testrock01-00.png'`).\n",
    "- **Inference and Logging**: The `infer_and_log()` function is called, which performs inference on the test image and logs the results to **W&B**.\n",
    "\n",
    "The predicted class is then printed to the console to verify the result.\n",
    "\n",
    "By using this function, you can easily evaluate the model's predictions on individual images and track those predictions through **Weights & Biases** for better analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: rock\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "# import wandb\n",
    "\n",
    "# Function to perform inference on a single image and log results to W&B\n",
    "def infer_and_log(model, image_path, transform, device, class_names):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = outputs.max(1)\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "    \n",
    "    # Log the input image and the prediction to W&B\n",
    "    # wandb.log({\n",
    "    #     \"Image\": wandb.Image(image, caption=f\"Predicted: {predicted_class}\"),\n",
    "    #     \"Prediction\": predicted_class\n",
    "    # })\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "# Assuming class_names = ['paper', 'rock', 'scissors']\n",
    "class_names = ['paper', 'rock', 'scissors']\n",
    "\n",
    "# Path to the test image\n",
    "image_path = 'rps-test-set/rock/testrock01-00.png'\n",
    "\n",
    "# Run inference and log results to W&B\n",
    "predicted_class = infer_and_log(model, image_path, transform, device, class_names)\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) System Workflow\n",
    "\n",
    "The process involves several key steps:\n",
    "\n",
    "1. **Document Upload**: The user uploads a document (PDF or DOCX).\n",
    "   \n",
    "2. **Document Processing**: \n",
    "   - If a PDF is uploaded, PyPDF2 extracts the text.\n",
    "   - If a DOCX is uploaded, python-docx extracts the text.\n",
    "\n",
    "3. **Text Splitting**: The extracted text is divided into chunks of 500 tokens, with a 50-token overlap.\n",
    "\n",
    "4. **Embedding**: A Sentence Transformer (e.g., 'all-MiniLM-L6-v2') encodes each chunk into vector embeddings.\n",
    "\n",
    "5. **FAISS Indexing**: The document embeddings are stored in a FAISS index for efficient retrieval.\n",
    "\n",
    "6. **Query Input**: The user inputs a query.\n",
    "\n",
    "7. **Query Embedding**: The query is embedded using the same Sentence Transformer model.\n",
    "\n",
    "8. **Similarity Search**: The system compares the query embedding with document embeddings, retrieving the top 5 most relevant chunks.\n",
    "\n",
    "9. **Answer Generation**: The retrieved chunks are used as context to generate an answer via a LLaMA model (e.g., \"Llama-3.1-8B-Instruct\").\n",
    "\n",
    "10. **Chat Interface**: The user interacts with the chatbot, which retrieves information from the documents and generates responses based on the query.\n",
    "\n",
    "<img src=\"RAG.png\" alt=\"RAG System\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install all the following Libraries before import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Replace with actual model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Set pad_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the Sentence Transformer model (embedding model)\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to process PDF files\n",
    "def process_pdf_file(file_obj):\n",
    "    text = ''\n",
    "    reader = PyPDF2.PdfReader(file_obj)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text\n",
    "    return text\n",
    "\n",
    "# Function to process DOCX files\n",
    "def process_docx_file(file_obj):\n",
    "    doc = Document(file_obj)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text(text, chunk_size=500, overlap=50):\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = ' '.join(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Function to process documents\n",
    "def process_document(file_obj):\n",
    "    if file_obj is None:\n",
    "        return None, \"No file uploaded.\"\n",
    "    \n",
    "    if file_obj.name.endswith('.pdf'):\n",
    "        text = process_pdf_file(file_obj)\n",
    "    elif file_obj.name.endswith('.docx'):\n",
    "        text = process_docx_file(file_obj)\n",
    "    else:\n",
    "        return None, \"Unsupported file type. Please upload a PDF or DOCX file.\"\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = split_text(text)\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    corpus_embeddings = embedder.encode(chunks, convert_to_numpy=True).astype('float32')\n",
    "    dimension = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(corpus_embeddings)\n",
    "    \n",
    "    return (chunks, index), None\n",
    "\n",
    "# Retrieve relevant text based on query\n",
    "def retrieve(query, index, documents, embedder, top_k=5):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_texts = [documents[i] for i in indices[0]]\n",
    "    \n",
    "    # Debugging: Print retrieved texts\n",
    "    print(\"Retrieved Texts:\", retrieved_texts)\n",
    "    \n",
    "    return retrieved_texts\n",
    "\n",
    "# Generate an answer using Llama model\n",
    "def generate_answer(conversation_history, query, retrieved_texts, tokenizer, model, max_input_length=4096, max_new_tokens=150):\n",
    "    # Limit conversation history to last 5 exchanges\n",
    "    limited_history = conversation_history[-5:]\n",
    "    history = \"\\n\".join([f\"User: {msg['user']}\\nBot: {msg['bot']}\" for msg in limited_history])\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Below is a conversation between a user and an assistant. The assistant should answer the user's questions based on the provided context.\\n\\n\"\n",
    "        f\"Conversation History:\\n{history}\\n\\n\"\n",
    "        f\"User: {query}\\n\"\n",
    "        f\"Context:\\n{context}\\n\"\n",
    "        f\"Bot:\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_input_length - max_new_tokens)\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Gradio interface functions\n",
    "def handle_upload(file):\n",
    "    document_data, error = process_document(file)\n",
    "    if error:\n",
    "        return None, error\n",
    "    return document_data, None\n",
    "\n",
    "def handle_conversation(conversation, file, user_input):\n",
    "    if file is None:\n",
    "        return conversation, \"Please upload a document first.\"\n",
    "    \n",
    "    documents, index = file\n",
    "    retrieved_texts = retrieve(user_input, index, documents, embedder)\n",
    "    answer = generate_answer(conversation, user_input, retrieved_texts, tokenizer, model)\n",
    "    \n",
    "    # Update conversation history\n",
    "    conversation.append({\"user\": user_input, \"bot\": answer})\n",
    "    \n",
    "    return conversation, None\n",
    "\n",
    "# Function to update the chatbot display\n",
    "def update_chat(conversation):\n",
    "    return [(msg[\"user\"], msg[\"bot\"]) for msg in conversation]\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸ“„ Document-Based Conversational Chatbot\")\n",
    "    gr.Markdown(\"Upload a PDF or DOCX document and start chatting with the bot about its content.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_upload = gr.File(label=\"Upload a PDF or DOCX document\", file_types=[\".pdf\", \".docx\"])\n",
    "        process_button = gr.Button(\"Process Document\")\n",
    "    \n",
    "    document_state = gr.State()\n",
    "    process_error = gr.Textbox(label=\"Error\", visible=False, lines=2)\n",
    "    \n",
    "    process_button.click(\n",
    "        fn=handle_upload,\n",
    "        inputs=file_upload,\n",
    "        outputs=[document_state, process_error]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"### Chat Interface\")\n",
    "    \n",
    "    # Chat Interface Components\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot()\n",
    "        with gr.Row():\n",
    "            user_input = gr.Textbox(label=\"You:\", placeholder=\"Type your message here...\", show_label=False)\n",
    "            send_button = gr.Button(\"Send\")\n",
    "    \n",
    "    conversation_state = gr.State([])  # Initialize empty conversation history\n",
    "    \n",
    "    send_button.click(\n",
    "        fn=handle_conversation,\n",
    "        inputs=[conversation_state, document_state, user_input],\n",
    "        outputs=[conversation_state, process_error]\n",
    "    )\n",
    "    \n",
    "    # Update the chatbot display based on conversation_state\n",
    "    conversation_state.change(\n",
    "        fn=update_chat,\n",
    "        inputs=conversation_state,\n",
    "        outputs=chatbot\n",
    "    )\n",
    "    \n",
    "    # Clear the input textbox after sending a message\n",
    "    send_button.click(\n",
    "        lambda: \"\",\n",
    "        inputs=None,\n",
    "        outputs=user_input\n",
    "    )\n",
    "    \n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

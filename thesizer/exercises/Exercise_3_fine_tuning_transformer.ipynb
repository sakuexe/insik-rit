{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72edec8c",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 3: Fine-Tuning Pretrained Transformer on Text Classification Task\n",
    "\n",
    "In this lab, you will apply the concepts learned by fine-tuning a pre-trained Transformer model on a text classification task using the Hugging Face `transformers` library.\n",
    "\n",
    "## Objectives:\n",
    "- Learn to load a pre-trained model from Hugging Face.\n",
    "- Fine-tune the model on a text classification dataset.\n",
    "- Evaluate and save the fine-tuned model.\n",
    "\n",
    "### Instructions:\n",
    "Follow the steps below to complete the lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16efac3",
   "metadata": {},
   "source": [
    "## Installing Necessary Libraries\n",
    "\n",
    "In this lab, we will use the following Python libraries:\n",
    "\n",
    "- **Transformers**: Hugging Face's `transformers` library provides pre-trained models for various Natural Language Processing (NLP) tasks. We will use this to load and fine-tune a pre-trained transformer model.\n",
    "  \n",
    "- **Datasets**: Hugging Face's `datasets` library allows easy access to a wide range of datasets and provides tools for efficient data processing.\n",
    "  \n",
    "- **Scikit-learn**: A widely-used library for machine learning, which includes tools for metrics, evaluation, and preprocessing. In this lab, we'll use it to calculate evaluation metrics such as accuracy, precision, recall, and F1 score.\n",
    "  \n",
    "- **Accelerate**: This library allows seamless usage of different hardware (CPU, GPU) for training, helping to speed up the training process by utilizing all available resources efficiently.\n",
    "\n",
    "You can install these libraries using the following command:\n",
    "\n",
    "```bash\n",
    "!pip install transformers datasets scikit-learn accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddeba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install necessary libraries\n",
    "!pip install transformers datasets scikit-learn accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bec9db",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dace2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88854d17",
   "metadata": {},
   "source": [
    "## Step 2: Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing tokenizer from Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f21caf",
   "metadata": {},
   "source": [
    "## Step 3: Setting up the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00486d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pre-trained model and Trainer utilities\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'].shuffle().select(range(1000)),\n",
    "    eval_dataset=tokenized_datasets['test'].shuffle().select(range(1000)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20f91d",
   "metadata": {},
   "source": [
    "## Step 4: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d43839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c5581",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686bfc2e",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "we will evaluate the performance of the fine-tuned model using the following metrics:\n",
    "\n",
    "- **Accuracy**: This measures the percentage of correctly predicted labels out of all predictions. It is useful for understanding the overall correctness of the model. \n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "    $$\n",
    "\n",
    "- **Precision**: Precision measures how many of the positive predictions made by the model are actually correct. It is especially important when the cost of false positives is high (i.e., incorrectly labeling negative examples as positive).\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "    $$\n",
    "\n",
    "- **Recall**: Recall (also known as sensitivity or true positive rate) measures how many actual positive cases the model is able to correctly identify. It is important when the cost of missing positive examples (false negatives) is high.\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "    $$\n",
    "\n",
    "- **F1 Score**: The F1 score is the harmonic mean of precision and recall, providing a balance between the two. It is particularly useful when you need to balance both false positives and false negatives.\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# Update trainer to include custom metrics\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684fc02",
   "metadata": {},
   "source": [
    "## Step 6: Saving the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f62810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model('my-fine-tuned-bert')\n",
    "tokenizer.save_pretrained('my-fine-tuned-bert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c54418",
   "metadata": {},
   "source": [
    "## Step 7: Loading and Testing the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed79bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load saved model and tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained('my-fine-tuned-bert')\n",
    "new_tokenizer = AutoTokenizer.from_pretrained('my-fine-tuned-bert')\n",
    "\n",
    "# Create a classification pipeline\n",
    "classifier = TextClassificationPipeline(model=new_model, tokenizer=new_tokenizer)\n",
    "\n",
    "# Add label mapping for sentiment analysis (assuming LABEL_0 = 'negative' and LABEL_1 = 'positive')\n",
    "label_mapping = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "# Test the model\n",
    "result = classifier(\"This movie was excellent.\")\n",
    "\n",
    "# Map the result to more meaningful labels\n",
    "mapped_result = {'label': label_mapping[int(result[0]['label'].split('_')[1])], 'score': result[0]['score']}\n",
    "print(mapped_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

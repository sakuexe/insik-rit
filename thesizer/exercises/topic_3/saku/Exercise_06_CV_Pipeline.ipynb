{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc6b9f2",
   "metadata": {},
   "source": [
    "# Exercise 6: Introduction to Computer Vision Pipelines in Hugging Face\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- Understand **Computer Vision Pipelines** in Hugging Face.\n",
    "- Learn how to create basic interfaces using **Gradio's components**.\n",
    "- Explore customization of interfaces with **themes and CSS**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9f132",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0528fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (4.44.1)\n",
      "Requirement already satisfied: transformers in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (4.6.0)\n",
      "Requirement already satisfied: fastapi<1.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.115.0)\n",
      "Requirement already satisfied: ffmpy in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (3.9.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: packaging in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.6.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio) (0.31.0)\n",
      "Requirement already satisfied: fsspec in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio-client==1.3.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: filelock in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from fastapi<1.0->gradio) (0.38.6)\n",
      "Requirement already satisfied: certifi in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (13.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio transformers timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e589f",
   "metadata": {},
   "source": [
    "# Computer Vision Pipelines in Hugging Face\n",
    "\n",
    "Hugging Face provides powerful and easy-to-use pipelines for various computer vision tasks. These pipelines simplify the process of loading pre-trained models and using them for tasks such as image classification, segmentation, and more. Below are some useful links to explore these pipelines:\n",
    "\n",
    "### CV Pipelines\n",
    "The Hugging Face Transformers library offers a variety of computer vision pipelines, making it easy to apply state-of-the-art models to images. For more information about the available pipelines, visit:\n",
    "[Computer Vision Pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines#computer-vision)\n",
    "\n",
    "### Image Classification\n",
    "To learn more about how to use pre-trained models for image classification tasks, refer to the following guide:\n",
    "[Task Guide: Image Classification with Transformers](https://huggingface.co/docs/transformers/en/tasks/image_classification)\n",
    "\n",
    "### Image Segmentation\n",
    "The image segmentation pipeline allows for identifying objects within an image at the pixel level. For details on how this pipeline works, check the following link:\n",
    "[Image Segmentation Pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.ImageSegmentationPipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b974ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50-panoptic were not used when initializing DetrForSegmentation: ['detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetrForSegmentation(\n",
      "  (detr): DetrForObjectDetection(\n",
      "    (model): DetrModel(\n",
      "      (backbone): DetrConvModel(\n",
      "        (conv_encoder): DetrConvEncoder(\n",
      "          (model): FeatureListNet(\n",
      "            (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "            (bn1): DetrFrozenBatchNorm2d()\n",
      "            (act1): ReLU(inplace=True)\n",
      "            (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "            (layer1): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "                (downsample): Sequential(\n",
      "                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                  (1): DetrFrozenBatchNorm2d()\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (layer2): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "                (downsample): Sequential(\n",
      "                  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                  (1): DetrFrozenBatchNorm2d()\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (3): Bottleneck(\n",
      "                (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (layer3): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "                (downsample): Sequential(\n",
      "                  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                  (1): DetrFrozenBatchNorm2d()\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (3): Bottleneck(\n",
      "                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (4): Bottleneck(\n",
      "                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (5): Bottleneck(\n",
      "                (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (layer4): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "                (downsample): Sequential(\n",
      "                  (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                  (1): DetrFrozenBatchNorm2d()\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "              (2): Bottleneck(\n",
      "                (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn1): DetrFrozenBatchNorm2d()\n",
      "                (act1): ReLU(inplace=True)\n",
      "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn2): DetrFrozenBatchNorm2d()\n",
      "                (drop_block): Identity()\n",
      "                (act2): ReLU(inplace=True)\n",
      "                (aa): Identity()\n",
      "                (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn3): DetrFrozenBatchNorm2d()\n",
      "                (act3): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (position_embedding): DetrSinePositionEmbedding()\n",
      "      )\n",
      "      (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (query_position_embeddings): Embedding(100, 256)\n",
      "      (encoder): DetrEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x DetrEncoderLayer(\n",
      "            (self_attn): DetrAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (activation_fn): ReLU()\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (decoder): DetrDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x DetrDecoderLayer(\n",
      "            (self_attn): DetrAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): DetrAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (class_labels_classifier): Linear(in_features=256, out_features=251, bias=True)\n",
      "    (bbox_predictor): DetrMLPPredictionHead(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_head): DetrMaskHeadSmallConv(\n",
      "    (lay1): Conv2d(264, 264, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (gn1): GroupNorm(8, 264, eps=1e-05, affine=True)\n",
      "    (lay2): Conv2d(264, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (gn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "    (lay3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (gn3): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "    (lay4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (gn4): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "    (lay5): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (gn5): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "    (out_lay): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (adapter1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (adapter2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (adapter3): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (bbox_attention): DetrMHAttentionMap(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n",
      "Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<PIL.Image.Image image mode=L size=1024x1024 at 0x7F380DE54D90>, 'cat (0.955574)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F3813813F10>, 'cat (0.960916)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F3813810D90>, 'LABEL_199 (0.99778)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F38344FD450>, 'LABEL_189 (0.985597)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F380DE705D0>, 'cat (0.983577)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F381385F110>, 'keyboard (0.996687)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F38138DD750>, 'cat (0.96524)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F38138DFCD0>, 'mouse (0.998612)')]\n",
      "[]\n",
      "[(<PIL.Image.Image image mode=L size=1024x1024 at 0x7F380DF32E90>, 'keyboard (0.990801)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F381A628410>, 'mouse (0.90346)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F380664BE10>, 'LABEL_189 (0.989015)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F3806648610>, 'tv (0.95046)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F380664A210>, 'cat (0.989407)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F380664A450>, 'keyboard (0.994296)'), (<PIL.Image.Image image mode=L size=1024x1024 at 0x7F3806649E10>, 'mouse (0.999761)')]\n",
      "[(<PIL.Image.Image image mode=L size=513x221 at 0x7F3806666810>, 'person (0.999544)'), (<PIL.Image.Image image mode=L size=513x221 at 0x7F3806620A10>, 'tie (0.967261)'), (<PIL.Image.Image image mode=L size=513x221 at 0x7F3806667FD0>, 'person (0.999554)')]\n",
      "[(<PIL.Image.Image image mode=L size=1000x684 at 0x7F3815CEFC90>, 'LABEL_193 (0.997608)'), (<PIL.Image.Image image mode=L size=1000x684 at 0x7F3815CEE750>, 'bird (0.989064)')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/components/image.py\", line 190, in preprocess\n",
      "    im = PIL.Image.open(file_path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/PIL/Image.py\", line 3498, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/b7099f5a1b80762e226b13dc05d811906c47438866bc378db133f437b4bc8c3f/little-fairy-shea-4x6-untitled-0182.avif'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/gradio/components/image.py\", line 190, in preprocess\n",
      "    im = PIL.Image.open(file_path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sakuk/code/ai_insikoorit/tree_recognition/venv/lib64/python3.11/site-packages/PIL/Image.py\", line 3498, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "PIL.UnidentifiedImageError: cannot identify image file '/tmp/gradio/b7099f5a1b80762e226b13dc05d811906c47438866bc378db133f437b4bc8c3f/little-fairy-shea-4x6-untitled-0182.avif'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<PIL.Image.Image image mode=L size=220x392 at 0x7F38138D6F50>, 'LABEL_190 (0.987721)'), (<PIL.Image.Image image mode=L size=220x392 at 0x7F38138D5FD0>, 'cat (0.999604)')]\n",
      "[(<PIL.Image.Image image mode=L size=6419x4279 at 0x7F3825BDAB50>, 'LABEL_199 (0.999447)'), (<PIL.Image.Image image mode=L size=6419x4279 at 0x7F38138C99D0>, 'LABEL_189 (0.960953)'), (<PIL.Image.Image image mode=L size=6419x4279 at 0x7F3825B03650>, 'bowl (0.902019)')]\n",
      "[(<PIL.Image.Image image mode=L size=107x224 at 0x7F3716BF5B50>, 'LABEL_187 (0.999473)'), (<PIL.Image.Image image mode=L size=107x224 at 0x7F38138C6810>, 'LABEL_184 (0.998866)'), (<PIL.Image.Image image mode=L size=107x224 at 0x7F38138FF1D0>, 'LABEL_193 (0.99585)')]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the image classification pipeline\n",
    "classifier = pipeline(\"image-segmentation\")\n",
    "# Alternatively you can define what model should the pipeline use, sometimes it requires that you login with your token\n",
    "# classifier = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50-panoptic\")\n",
    "\n",
    "def segmentate_image(image) -> list[tuple[Image.Image, str]]:\n",
    "    results = classifier(image)\n",
    "    # Get the top prediction\n",
    "    images = [\n",
    "        (\n",
    "            result['mask'],\n",
    "            f\"{result['label']} ({result['score']})\"\n",
    "        )\n",
    "        for result in results\n",
    "    ]\n",
    "    return images\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Select an image\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(label=\"Upload image\", type=\"pil\")\n",
    "            submit_button = gr.Button(\"submit\")\n",
    "\n",
    "        with gr.Column():\n",
    "            result_output = gr.Gallery(\n",
    "                label=\"Masks\",\n",
    "                columns=3,\n",
    "                rows=2, \n",
    "                object_fit=\"contain\",\n",
    "                height=\"auto\",\n",
    "                show_download_button=True\n",
    "                )\n",
    "\n",
    "            submit_button.click(\n",
    "                fn=segmentate_image, \n",
    "                inputs=image_input,\n",
    "                outputs=result_output\n",
    "                )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb17bd7",
   "metadata": {},
   "source": [
    "## Some Basic Gradio Stuff\n",
    "#### Building a Simple Calculator\n",
    "\n",
    "In this exercise, we'll build a simple calculator using Gradio.\n",
    "\n",
    "\n",
    "- Create a Gradio interface that takes two numbers and an operation (Add, Subtract, Multiply, Divide).\n",
    "- The interface should display the result of the calculation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def calculator(a, b, operation):\n",
    "    if operation == 'Add':\n",
    "        return a + b\n",
    "    elif operation == 'Subtract':\n",
    "        return a - b\n",
    "    elif operation == 'Multiply':\n",
    "        return a * b\n",
    "    elif operation == 'Divide':\n",
    "        if b != 0:\n",
    "            return a / b\n",
    "        else:\n",
    "            return 'Error: Division by zero'\n",
    "    else:\n",
    "        return 'Invalid operation'\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=calculator,\n",
    "    inputs=[\n",
    "        gr.Number(label=\"Input A\"),\n",
    "        gr.Number(label=\"Input B\"),\n",
    "        gr.Radio(['Add', 'Subtract', 'Multiply', 'Divide'], label=\"Operation\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Result\"),\n",
    "    title=\"Simple Calculator\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7d388",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- **Modify the calculator** to include more operations, such as exponentiation or modulus.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48381a9",
   "metadata": {},
   "source": [
    "#### Creating a BMI Calculator\n",
    "\n",
    "In this exercise, we'll create a Body Mass Index (BMI) calculator.\n",
    "\n",
    "\n",
    "- Build a Gradio interface that takes a user's weight and height.\n",
    "- The tool should calculate and display the user's BMI.\n",
    "- Additionally, it should provide a simple interpretation of the BMI value (e.g., Underweight, Normal weight, Overweight, Obesity).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ab1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def calculate_bmi(weight, height):\n",
    "    try:\n",
    "        bmi = weight / ((height / 100) ** 2)\n",
    "        bmi = round(bmi, 2)\n",
    "        if bmi < 18.5:\n",
    "            interpretation = \"Underweight\"\n",
    "        elif 18.5 <= bmi < 25:\n",
    "            interpretation = \"Normal weight\"\n",
    "        elif 25 <= bmi < 30:\n",
    "            interpretation = \"Overweight\"\n",
    "        else:\n",
    "            interpretation = \"Obesity\"\n",
    "        return f\"BMI: {bmi}, Interpretation: {interpretation}\"\n",
    "    except ZeroDivisionError:\n",
    "        return \"Error: Height cannot be zero.\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=calculate_bmi,\n",
    "    inputs=[\n",
    "        gr.Number(label=\"Weight (kg)\"),\n",
    "        gr.Number(label=\"Height (cm)\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"BMI Result\"),\n",
    "    title=\"BMI Calculator\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a4529",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- **Improve the BMI Calculator** by adding input validation (e.g., ensure that weight and height are positive numbers).\n",
    "- **Enhance the interface** by providing additional health tips based on the BMI category.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364e51e",
   "metadata": {},
   "source": [
    "#### Customizing the Interface\n",
    "\n",
    "Let's customize the appearance of our Gradio apps.\n",
    "\n",
    "\n",
    "\n",
    "- Apply a theme to your app.\n",
    "- Add custom CSS to hide the Gradio footer.\n",
    "\n",
    "\n",
    "\n",
    "Using the BMI Calculator from the previous exercise, let's apply a theme and custom CSS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_css = \"\"\"\n",
    ".footer {display: none !important;}\n",
    "\"\"\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=calculate_bmi,\n",
    "    inputs=[\n",
    "        gr.Number(label=\"Weight (kg)\"),\n",
    "        gr.Number(label=\"Height (cm)\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"BMI Result\"),\n",
    "    title=\"BMI Calculator\",\n",
    "    theme=\"default\",  # You can choose from \"default\", \"huggingface\", \"grass\", \"dark\", etc.\n",
    "    css=custom_css\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa521d",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "- **Change the layout** using `gr.Blocks`.\n",
    "- **Add an image or logo** to your app.\n",
    "\n",
    "#### Using `gr.Blocks` to Change Layout:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d17a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=\"default\", css=custom_css) as demo:\n",
    "    gr.Markdown(\"# BMI Calculator\")\n",
    "    gr.Markdown(\"Calculate your Body Mass Index (BMI).\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            weight_input = gr.Number(label=\"Weight (kg)\")\n",
    "            height_input = gr.Number(label=\"Height (cm)\")\n",
    "            calculate_button = gr.Button(\"Calculate BMI\")\n",
    "        with gr.Column():\n",
    "            bmi_output = gr.Textbox(label=\"BMI Result\")\n",
    "    calculate_button.click(\n",
    "        fn=calculate_bmi,\n",
    "        inputs=[weight_input, height_input],\n",
    "        outputs=bmi_output\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea64d78",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Temperature Converter\n",
    "\n",
    "Create a Gradio app that converts temperatures between Celsius, Fahrenheit, and Kelvin.\n",
    "\n",
    "\n",
    "\n",
    "- The app should have:\n",
    "  - An input field for the temperature value.\n",
    "  - A dropdown to select the input unit (Celsius, Fahrenheit, Kelvin).\n",
    "  - A dropdown to select the output unit.\n",
    "- The app should display the converted temperature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c676d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_temperature(value, input_unit, output_unit):\n",
    "    value = float(value)\n",
    "    if input_unit == output_unit:\n",
    "        return value\n",
    "    # Convert input to Celsius\n",
    "    if input_unit == 'Celsius':\n",
    "        celsius = value\n",
    "    elif input_unit == 'Fahrenheit':\n",
    "        celsius = (value - 32) * 5/9\n",
    "    elif input_unit == 'Kelvin':\n",
    "        celsius = value - 273.15\n",
    "    else:\n",
    "        return 'Invalid input unit'\n",
    "    # Convert Celsius to output unit\n",
    "    if output_unit == 'Celsius':\n",
    "        return celsius\n",
    "    elif output_unit == 'Fahrenheit':\n",
    "        return celsius * 9/5 + 32\n",
    "    elif output_unit == 'Kelvin':\n",
    "        return celsius + 273.15\n",
    "    else:\n",
    "        return 'Invalid output unit'\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=convert_temperature,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Temperature Value\"),\n",
    "        gr.Dropdown(['Celsius', 'Fahrenheit', 'Kelvin'], label=\"Input Unit\"),\n",
    "        gr.Dropdown(['Celsius', 'Fahrenheit', 'Kelvin'], label=\"Output Unit\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Converted Temperature\"),\n",
    "    title=\"Temperature Converter\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
